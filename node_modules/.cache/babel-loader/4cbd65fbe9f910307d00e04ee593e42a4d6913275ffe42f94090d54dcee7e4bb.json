{"ast":null,"code":"import { Configuration, OpenAIApi } from \"openai\";\nimport { calculateMaxTokens } from \"../base_language/count_tokens.js\";\nimport { GenerationChunk } from \"../schema/index.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { getEndpoint } from \"../util/azure.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { getEnvironmentVariable, isNode } from \"../util/env.js\";\nimport { promptLayerTrackRequest } from \"../util/prompt-layer.js\";\nimport { readableStreamToAsyncIterable } from \"../util/stream.js\";\nimport { BaseLLM } from \"./base.js\";\nimport { OpenAIChat } from \"./openai-chat.js\";\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nexport class OpenAI extends BaseLLM {\n  static lc_name() {\n    return \"OpenAI\";\n  }\n  get callKeys() {\n    return [...super.callKeys, \"options\"];\n  }\n  get lc_secrets() {\n    return {\n      openAIApiKey: \"OPENAI_API_KEY\",\n      azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\"\n    };\n  }\n  get lc_aliases() {\n    return {\n      modelName: \"model\",\n      openAIApiKey: \"openai_api_key\",\n      azureOpenAIApiVersion: \"azure_openai_api_version\",\n      azureOpenAIApiKey: \"azure_openai_api_key\",\n      azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n      azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\"\n    };\n  }\n  constructor(fields, /** @deprecated */\n  configuration) {\n    if (fields?.modelName?.startsWith(\"gpt-3.5-turbo\") || fields?.modelName?.startsWith(\"gpt-4\") || fields?.modelName?.startsWith(\"gpt-4-32k\")) {\n      // eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\n      return new OpenAIChat(fields, configuration);\n    }\n    super(fields ?? {});\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: true\n    });\n    Object.defineProperty(this, \"temperature\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0.7\n    });\n    Object.defineProperty(this, \"maxTokens\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 256\n    });\n    Object.defineProperty(this, \"topP\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"frequencyPenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"presencePenalty\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 0\n    });\n    Object.defineProperty(this, \"n\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 1\n    });\n    Object.defineProperty(this, \"bestOf\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"logitBias\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"modelName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: \"text-davinci-003\"\n    });\n    Object.defineProperty(this, \"modelKwargs\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"batchSize\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: 20\n    });\n    Object.defineProperty(this, \"timeout\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"stop\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"user\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"streaming\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"openAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"azureOpenAIBasePath\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"organization\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"client\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"clientConfig\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.openAIApiKey = fields?.openAIApiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.azureOpenAIApiKey = fields?.azureOpenAIApiKey ?? getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n    if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n      throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n    }\n    this.azureOpenAIApiInstanceName = fields?.azureOpenAIApiInstanceName ?? getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n    this.azureOpenAIApiDeploymentName = (fields?.azureOpenAIApiCompletionsDeploymentName || fields?.azureOpenAIApiDeploymentName) ?? (getEnvironmentVariable(\"AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\") || getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n    this.azureOpenAIApiVersion = fields?.azureOpenAIApiVersion ?? getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n    this.azureOpenAIBasePath = fields?.azureOpenAIBasePath ?? getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n    this.organization = fields?.configuration?.organization ?? getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n    this.modelName = fields?.modelName ?? this.modelName;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.batchSize = fields?.batchSize ?? this.batchSize;\n    this.timeout = fields?.timeout;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.n = fields?.n ?? this.n;\n    this.bestOf = fields?.bestOf ?? this.bestOf;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stop;\n    this.user = fields?.user;\n    this.streaming = fields?.streaming ?? false;\n    if (this.streaming && this.bestOf && this.bestOf > 1) {\n      throw new Error(\"Cannot stream results when bestOf > 1\");\n    }\n    if (this.azureOpenAIApiKey) {\n      if (!this.azureOpenAIApiInstanceName && !this.azureOpenAIBasePath) {\n        throw new Error(\"Azure OpenAI API instance name not found\");\n      }\n      if (!this.azureOpenAIApiDeploymentName) {\n        throw new Error(\"Azure OpenAI API deployment name not found\");\n      }\n      if (!this.azureOpenAIApiVersion) {\n        throw new Error(\"Azure OpenAI API version not found\");\n      }\n    }\n    this.clientConfig = {\n      apiKey: this.openAIApiKey,\n      organization: this.organization,\n      ...configuration,\n      ...fields?.configuration\n    };\n  }\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(options) {\n    return {\n      model: this.modelName,\n      temperature: this.temperature,\n      max_tokens: this.maxTokens,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      n: this.n,\n      best_of: this.bestOf,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stop,\n      user: this.user,\n      stream: this.streaming,\n      ...this.modelKwargs\n    };\n  }\n  _identifyingParams() {\n    return {\n      model_name: this.modelName,\n      ...this.invocationParams(),\n      ...this.clientConfig\n    };\n  }\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n  /**\n   * Call out to OpenAI's endpoint with k unique prompts\n   *\n   * @param [prompts] - The prompts to pass into the model.\n   * @param [options] - Optional list of stop words to use when generating.\n   * @param [runManager] - Optional callback manager to use when generating.\n   *\n   * @returns The full LLM output.\n   *\n   * @example\n   * ```ts\n   * import { OpenAI } from \"langchain/llms/openai\";\n   * const openai = new OpenAI();\n   * const response = await openai.generate([\"Tell me a joke.\"]);\n   * ```\n   */\n  async _generate(prompts, options, runManager) {\n    const subPrompts = chunkArray(prompts, this.batchSize);\n    const choices = [];\n    const tokenUsage = {};\n    const params = this.invocationParams(options);\n    if (params.max_tokens === -1) {\n      if (prompts.length !== 1) {\n        throw new Error(\"max_tokens set to -1 not supported for multiple inputs\");\n      }\n      params.max_tokens = await calculateMaxTokens({\n        prompt: prompts[0],\n        // Cast here to allow for other models that may not fit the union\n        modelName: this.modelName\n      });\n    }\n    for (let i = 0; i < subPrompts.length; i += 1) {\n      const data = params.stream ? await new Promise((resolve, reject) => {\n        const choices = [];\n        let response;\n        let rejected = false;\n        let resolved = false;\n        this.completionWithRetry({\n          ...params,\n          prompt: subPrompts[i]\n        }, {\n          signal: options.signal,\n          ...options.options,\n          adapter: fetchAdapter,\n          responseType: \"stream\",\n          onmessage: event => {\n            if (event.data?.trim?.() === \"[DONE]\") {\n              if (resolved || rejected) {\n                return;\n              }\n              resolved = true;\n              resolve({\n                ...response,\n                choices\n              });\n            } else {\n              const data = JSON.parse(event.data);\n              if (data?.error) {\n                if (rejected) {\n                  return;\n                }\n                rejected = true;\n                reject(data.error);\n                return;\n              }\n              const message = data;\n              // on the first message set the response properties\n              if (!response) {\n                response = {\n                  id: message.id,\n                  object: message.object,\n                  created: message.created,\n                  model: message.model\n                };\n              }\n              // on all messages, update choice\n              for (const part of message.choices) {\n                if (part != null && part.index != null) {\n                  if (!choices[part.index]) choices[part.index] = {};\n                  const choice = choices[part.index];\n                  choice.text = (choice.text ?? \"\") + (part.text ?? \"\");\n                  choice.finish_reason = part.finish_reason;\n                  choice.logprobs = part.logprobs;\n                  // eslint-disable-next-line no-void\n                  void runManager?.handleLLMNewToken(part.text ?? \"\", {\n                    prompt: Math.floor(part.index / this.n),\n                    completion: part.index % this.n\n                  });\n                }\n              }\n              // when all messages are finished, resolve\n              if (!resolved && !rejected && choices.every(c => c.finish_reason != null)) {\n                resolved = true;\n                resolve({\n                  ...response,\n                  choices\n                });\n              }\n            }\n          }\n        }).catch(error => {\n          if (!rejected) {\n            rejected = true;\n            reject(error);\n          }\n        });\n      }) : await this.completionWithRetry({\n        ...params,\n        prompt: subPrompts[i]\n      }, {\n        signal: options.signal,\n        ...options.options\n      });\n      choices.push(...data.choices);\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens\n      } = data.usage ?? {};\n      if (completionTokens) {\n        tokenUsage.completionTokens = (tokenUsage.completionTokens ?? 0) + completionTokens;\n      }\n      if (promptTokens) {\n        tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n      }\n      if (totalTokens) {\n        tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n      }\n    }\n    const generations = chunkArray(choices, this.n).map(promptChoices => promptChoices.map(choice => ({\n      text: choice.text ?? \"\",\n      generationInfo: {\n        finishReason: choice.finish_reason,\n        logprobs: choice.logprobs\n      }\n    })));\n    return {\n      generations,\n      llmOutput: {\n        tokenUsage\n      }\n    };\n  }\n  // TODO(jacoblee): Refactor with _generate(..., {stream: true}) implementation\n  // when we integrate OpenAI's new SDK.\n  async *_streamResponseChunks(input, options, runManager) {\n    const params = {\n      ...this.invocationParams(options),\n      prompt: input,\n      stream: true\n    };\n    const streamIterable = this.startStream(params, options);\n    for await (const streamedResponse of streamIterable) {\n      const data = JSON.parse(streamedResponse);\n      const choice = data.choices?.[0];\n      if (!choice) {\n        continue;\n      }\n      const chunk = new GenerationChunk({\n        text: choice.text,\n        generationInfo: {\n          finishReason: choice.finish_reason,\n          logprobs: choice.logprobs\n        }\n      });\n      yield chunk;\n      // eslint-disable-next-line no-void\n      void runManager?.handleLLMNewToken(chunk.text ?? \"\");\n    }\n  }\n  startStream(request, options) {\n    let done = false;\n    const stream = new TransformStream();\n    const writer = stream.writable.getWriter();\n    const iterable = readableStreamToAsyncIterable(stream.readable);\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    let err;\n    this.completionWithRetry(request, {\n      ...options,\n      adapter: fetchAdapter,\n      responseType: \"stream\",\n      onmessage: event => {\n        if (done) return;\n        if (event.data?.trim?.() === \"[DONE]\") {\n          done = true;\n          // eslint-disable-next-line no-void\n          void writer.close();\n        } else {\n          const data = JSON.parse(event.data);\n          if (data.error) {\n            done = true;\n            throw data.error;\n          }\n          // eslint-disable-next-line no-void\n          void writer.write(event.data);\n        }\n      }\n    }).catch(error => {\n      if (!done) {\n        err = error;\n        done = true;\n        // eslint-disable-next-line no-void\n        void writer.close();\n      }\n    });\n    return {\n      async next() {\n        const chunk = await iterable.next();\n        if (err) {\n          throw err;\n        }\n        return chunk;\n      },\n      [Symbol.asyncIterator]() {\n        return this;\n      }\n    };\n  }\n  /** @ignore */\n  async completionWithRetry(request, options) {\n    if (!this.client) {\n      const openAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        basePath: this.clientConfig.basePath\n      };\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const clientConfig = new Configuration({\n        ...this.clientConfig,\n        basePath: endpoint,\n        baseOptions: {\n          timeout: this.timeout,\n          ...this.clientConfig.baseOptions\n        }\n      });\n      this.client = new OpenAIApi(clientConfig);\n    }\n    const axiosOptions = {\n      adapter: isNode() ? undefined : fetchAdapter,\n      ...this.clientConfig.baseOptions,\n      ...options\n    };\n    if (this.azureOpenAIApiKey) {\n      axiosOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...axiosOptions.headers\n      };\n      axiosOptions.params = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...axiosOptions.params\n      };\n    }\n    return this.caller.call(this.client.createCompletion.bind(this.client), request, axiosOptions).then(res => res.data);\n  }\n  _llmType() {\n    return \"openai\";\n  }\n}\n/**\n * PromptLayer wrapper to OpenAI\n * @augments OpenAI\n */\nexport class PromptLayerOpenAI extends OpenAI {\n  get lc_secrets() {\n    return {\n      promptLayerApiKey: \"PROMPTLAYER_API_KEY\"\n    };\n  }\n  constructor(fields) {\n    super(fields);\n    Object.defineProperty(this, \"lc_serializable\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: false\n    });\n    Object.defineProperty(this, \"promptLayerApiKey\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"plTags\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    Object.defineProperty(this, \"returnPromptLayerId\", {\n      enumerable: true,\n      configurable: true,\n      writable: true,\n      value: void 0\n    });\n    this.plTags = fields?.plTags ?? [];\n    this.promptLayerApiKey = fields?.promptLayerApiKey ?? getEnvironmentVariable(\"PROMPTLAYER_API_KEY\");\n    this.returnPromptLayerId = fields?.returnPromptLayerId;\n    if (!this.promptLayerApiKey) {\n      throw new Error(\"Missing PromptLayer API key\");\n    }\n  }\n  async completionWithRetry(request, options) {\n    if (request.stream) {\n      return super.completionWithRetry(request, options);\n    }\n    const response = await super.completionWithRetry(request);\n    return response;\n  }\n  async _generate(prompts, options, runManager) {\n    const requestStartTime = Date.now();\n    const generations = await super._generate(prompts, options, runManager);\n    for (let i = 0; i < generations.generations.length; i += 1) {\n      const requestEndTime = Date.now();\n      const parsedResp = {\n        text: generations.generations[i][0].text,\n        llm_output: generations.llmOutput\n      };\n      const promptLayerRespBody = await promptLayerTrackRequest(this.caller, \"langchain.PromptLayerOpenAI\", [prompts[i]], this._identifyingParams(), this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n      let promptLayerRequestId;\n      if (this.returnPromptLayerId === true) {\n        if (promptLayerRespBody && promptLayerRespBody.success === true) {\n          promptLayerRequestId = promptLayerRespBody.request_id;\n        }\n        generations.generations[i][0].generationInfo = {\n          ...generations.generations[i][0].generationInfo,\n          promptLayerRequestId\n        };\n      }\n    }\n    return generations;\n  }\n}\nexport { OpenAIChat, PromptLayerOpenAIChat } from \"./openai-chat.js\";","map":{"version":3,"names":["Configuration","OpenAIApi","calculateMaxTokens","GenerationChunk","fetchAdapter","getEndpoint","chunkArray","getEnvironmentVariable","isNode","promptLayerTrackRequest","readableStreamToAsyncIterable","BaseLLM","OpenAIChat","OpenAI","lc_name","callKeys","lc_secrets","openAIApiKey","azureOpenAIApiKey","organization","lc_aliases","modelName","azureOpenAIApiVersion","azureOpenAIApiInstanceName","azureOpenAIApiDeploymentName","constructor","fields","configuration","startsWith","Object","defineProperty","enumerable","configurable","writable","value","Error","azureOpenAIApiCompletionsDeploymentName","azureOpenAIBasePath","modelKwargs","batchSize","timeout","temperature","maxTokens","topP","frequencyPenalty","presencePenalty","n","bestOf","logitBias","stop","user","streaming","clientConfig","apiKey","invocationParams","options","model","max_tokens","top_p","frequency_penalty","presence_penalty","best_of","logit_bias","stream","_identifyingParams","model_name","identifyingParams","_generate","prompts","runManager","subPrompts","choices","tokenUsage","params","length","prompt","i","data","Promise","resolve","reject","response","rejected","resolved","completionWithRetry","signal","adapter","responseType","onmessage","event","trim","JSON","parse","error","message","id","object","created","part","index","choice","text","finish_reason","logprobs","handleLLMNewToken","Math","floor","completion","every","c","catch","push","completion_tokens","completionTokens","prompt_tokens","promptTokens","total_tokens","totalTokens","usage","generations","map","promptChoices","generationInfo","finishReason","llmOutput","_streamResponseChunks","input","streamIterable","startStream","streamedResponse","chunk","request","done","TransformStream","writer","getWriter","iterable","readable","err","close","write","next","Symbol","asyncIterator","client","openAIEndpointConfig","basePath","endpoint","baseOptions","axiosOptions","undefined","headers","caller","call","createCompletion","bind","then","res","_llmType","PromptLayerOpenAI","promptLayerApiKey","plTags","returnPromptLayerId","requestStartTime","Date","now","requestEndTime","parsedResp","llm_output","promptLayerRespBody","promptLayerRequestId","success","request_id","PromptLayerOpenAIChat"],"sources":["/Users/idasilfverskiold/gptSandbox/reactlangchain/sandbox/node_modules/langchain/dist/llms/openai.js"],"sourcesContent":["import { Configuration, OpenAIApi, } from \"openai\";\nimport { calculateMaxTokens } from \"../base_language/count_tokens.js\";\nimport { GenerationChunk } from \"../schema/index.js\";\nimport fetchAdapter from \"../util/axios-fetch-adapter.js\";\nimport { getEndpoint } from \"../util/azure.js\";\nimport { chunkArray } from \"../util/chunk.js\";\nimport { getEnvironmentVariable, isNode } from \"../util/env.js\";\nimport { promptLayerTrackRequest } from \"../util/prompt-layer.js\";\nimport { readableStreamToAsyncIterable } from \"../util/stream.js\";\nimport { BaseLLM } from \"./base.js\";\nimport { OpenAIChat } from \"./openai-chat.js\";\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nexport class OpenAI extends BaseLLM {\n    static lc_name() {\n        return \"OpenAI\";\n    }\n    get callKeys() {\n        return [...super.callKeys, \"options\"];\n    }\n    get lc_secrets() {\n        return {\n            openAIApiKey: \"OPENAI_API_KEY\",\n            azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n            organization: \"OPENAI_ORGANIZATION\",\n        };\n    }\n    get lc_aliases() {\n        return {\n            modelName: \"model\",\n            openAIApiKey: \"openai_api_key\",\n            azureOpenAIApiVersion: \"azure_openai_api_version\",\n            azureOpenAIApiKey: \"azure_openai_api_key\",\n            azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n            azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\",\n        };\n    }\n    constructor(fields, \n    /** @deprecated */\n    configuration) {\n        if (fields?.modelName?.startsWith(\"gpt-3.5-turbo\") ||\n            fields?.modelName?.startsWith(\"gpt-4\") ||\n            fields?.modelName?.startsWith(\"gpt-4-32k\")) {\n            // eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\n            return new OpenAIChat(fields, configuration);\n        }\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0.7\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 256\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"bestOf\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text-davinci-003\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"batchSize\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 20\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"user\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"openAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIBasePath\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"organization\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.openAIApiKey =\n            fields?.openAIApiKey ?? getEnvironmentVariable(\"OPENAI_API_KEY\");\n        this.azureOpenAIApiKey =\n            fields?.azureOpenAIApiKey ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n        if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n            throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n        }\n        this.azureOpenAIApiInstanceName =\n            fields?.azureOpenAIApiInstanceName ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n        this.azureOpenAIApiDeploymentName =\n            (fields?.azureOpenAIApiCompletionsDeploymentName ||\n                fields?.azureOpenAIApiDeploymentName) ??\n                (getEnvironmentVariable(\"AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\") ||\n                    getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n        this.azureOpenAIApiVersion =\n            fields?.azureOpenAIApiVersion ??\n                getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n        this.azureOpenAIBasePath =\n            fields?.azureOpenAIBasePath ??\n                getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n        this.organization =\n            fields?.configuration?.organization ??\n                getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.batchSize = fields?.batchSize ?? this.batchSize;\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.n = fields?.n ?? this.n;\n        this.bestOf = fields?.bestOf ?? this.bestOf;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.user = fields?.user;\n        this.streaming = fields?.streaming ?? false;\n        if (this.streaming && this.bestOf && this.bestOf > 1) {\n            throw new Error(\"Cannot stream results when bestOf > 1\");\n        }\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName && !this.azureOpenAIBasePath) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n        }\n        this.clientConfig = {\n            apiKey: this.openAIApiKey,\n            organization: this.organization,\n            ...configuration,\n            ...fields?.configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(options) {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            max_tokens: this.maxTokens,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            n: this.n,\n            best_of: this.bestOf,\n            logit_bias: this.logitBias,\n            stop: options?.stop ?? this.stop,\n            user: this.user,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /**\n     * Call out to OpenAI's endpoint with k unique prompts\n     *\n     * @param [prompts] - The prompts to pass into the model.\n     * @param [options] - Optional list of stop words to use when generating.\n     * @param [runManager] - Optional callback manager to use when generating.\n     *\n     * @returns The full LLM output.\n     *\n     * @example\n     * ```ts\n     * import { OpenAI } from \"langchain/llms/openai\";\n     * const openai = new OpenAI();\n     * const response = await openai.generate([\"Tell me a joke.\"]);\n     * ```\n     */\n    async _generate(prompts, options, runManager) {\n        const subPrompts = chunkArray(prompts, this.batchSize);\n        const choices = [];\n        const tokenUsage = {};\n        const params = this.invocationParams(options);\n        if (params.max_tokens === -1) {\n            if (prompts.length !== 1) {\n                throw new Error(\"max_tokens set to -1 not supported for multiple inputs\");\n            }\n            params.max_tokens = await calculateMaxTokens({\n                prompt: prompts[0],\n                // Cast here to allow for other models that may not fit the union\n                modelName: this.modelName,\n            });\n        }\n        for (let i = 0; i < subPrompts.length; i += 1) {\n            const data = params.stream\n                ? await new Promise((resolve, reject) => {\n                    const choices = [];\n                    let response;\n                    let rejected = false;\n                    let resolved = false;\n                    this.completionWithRetry({\n                        ...params,\n                        prompt: subPrompts[i],\n                    }, {\n                        signal: options.signal,\n                        ...options.options,\n                        adapter: fetchAdapter,\n                        responseType: \"stream\",\n                        onmessage: (event) => {\n                            if (event.data?.trim?.() === \"[DONE]\") {\n                                if (resolved || rejected) {\n                                    return;\n                                }\n                                resolved = true;\n                                resolve({\n                                    ...response,\n                                    choices,\n                                });\n                            }\n                            else {\n                                const data = JSON.parse(event.data);\n                                if (data?.error) {\n                                    if (rejected) {\n                                        return;\n                                    }\n                                    rejected = true;\n                                    reject(data.error);\n                                    return;\n                                }\n                                const message = data;\n                                // on the first message set the response properties\n                                if (!response) {\n                                    response = {\n                                        id: message.id,\n                                        object: message.object,\n                                        created: message.created,\n                                        model: message.model,\n                                    };\n                                }\n                                // on all messages, update choice\n                                for (const part of message.choices) {\n                                    if (part != null && part.index != null) {\n                                        if (!choices[part.index])\n                                            choices[part.index] = {};\n                                        const choice = choices[part.index];\n                                        choice.text = (choice.text ?? \"\") + (part.text ?? \"\");\n                                        choice.finish_reason = part.finish_reason;\n                                        choice.logprobs = part.logprobs;\n                                        // eslint-disable-next-line no-void\n                                        void runManager?.handleLLMNewToken(part.text ?? \"\", {\n                                            prompt: Math.floor(part.index / this.n),\n                                            completion: part.index % this.n,\n                                        });\n                                    }\n                                }\n                                // when all messages are finished, resolve\n                                if (!resolved &&\n                                    !rejected &&\n                                    choices.every((c) => c.finish_reason != null)) {\n                                    resolved = true;\n                                    resolve({\n                                        ...response,\n                                        choices,\n                                    });\n                                }\n                            }\n                        },\n                    }).catch((error) => {\n                        if (!rejected) {\n                            rejected = true;\n                            reject(error);\n                        }\n                    });\n                })\n                : await this.completionWithRetry({\n                    ...params,\n                    prompt: subPrompts[i],\n                }, {\n                    signal: options.signal,\n                    ...options.options,\n                });\n            choices.push(...data.choices);\n            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage ?? {};\n            if (completionTokens) {\n                tokenUsage.completionTokens =\n                    (tokenUsage.completionTokens ?? 0) + completionTokens;\n            }\n            if (promptTokens) {\n                tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n            }\n            if (totalTokens) {\n                tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n            }\n        }\n        const generations = chunkArray(choices, this.n).map((promptChoices) => promptChoices.map((choice) => ({\n            text: choice.text ?? \"\",\n            generationInfo: {\n                finishReason: choice.finish_reason,\n                logprobs: choice.logprobs,\n            },\n        })));\n        return {\n            generations,\n            llmOutput: { tokenUsage },\n        };\n    }\n    // TODO(jacoblee): Refactor with _generate(..., {stream: true}) implementation\n    // when we integrate OpenAI's new SDK.\n    async *_streamResponseChunks(input, options, runManager) {\n        const params = {\n            ...this.invocationParams(options),\n            prompt: input,\n            stream: true,\n        };\n        const streamIterable = this.startStream(params, options);\n        for await (const streamedResponse of streamIterable) {\n            const data = JSON.parse(streamedResponse);\n            const choice = data.choices?.[0];\n            if (!choice) {\n                continue;\n            }\n            const chunk = new GenerationChunk({\n                text: choice.text,\n                generationInfo: {\n                    finishReason: choice.finish_reason,\n                    logprobs: choice.logprobs,\n                },\n            });\n            yield chunk;\n            // eslint-disable-next-line no-void\n            void runManager?.handleLLMNewToken(chunk.text ?? \"\");\n        }\n    }\n    startStream(request, options) {\n        let done = false;\n        const stream = new TransformStream();\n        const writer = stream.writable.getWriter();\n        const iterable = readableStreamToAsyncIterable(stream.readable);\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        let err;\n        this.completionWithRetry(request, {\n            ...options,\n            adapter: fetchAdapter,\n            responseType: \"stream\",\n            onmessage: (event) => {\n                if (done)\n                    return;\n                if (event.data?.trim?.() === \"[DONE]\") {\n                    done = true;\n                    // eslint-disable-next-line no-void\n                    void writer.close();\n                }\n                else {\n                    const data = JSON.parse(event.data);\n                    if (data.error) {\n                        done = true;\n                        throw data.error;\n                    }\n                    // eslint-disable-next-line no-void\n                    void writer.write(event.data);\n                }\n            },\n        }).catch((error) => {\n            if (!done) {\n                err = error;\n                done = true;\n                // eslint-disable-next-line no-void\n                void writer.close();\n            }\n        });\n        return {\n            async next() {\n                const chunk = await iterable.next();\n                if (err) {\n                    throw err;\n                }\n                return chunk;\n            },\n            [Symbol.asyncIterator]() {\n                return this;\n            },\n        };\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const openAIEndpointConfig = {\n                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n                azureOpenAIApiKey: this.azureOpenAIApiKey,\n                azureOpenAIBasePath: this.azureOpenAIBasePath,\n                basePath: this.clientConfig.basePath,\n            };\n            const endpoint = getEndpoint(openAIEndpointConfig);\n            const clientConfig = new Configuration({\n                ...this.clientConfig,\n                basePath: endpoint,\n                baseOptions: {\n                    timeout: this.timeout,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new OpenAIApi(clientConfig);\n        }\n        const axiosOptions = {\n            adapter: isNode() ? undefined : fetchAdapter,\n            ...this.clientConfig.baseOptions,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            axiosOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...axiosOptions.headers,\n            };\n            axiosOptions.params = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...axiosOptions.params,\n            };\n        }\n        return this.caller\n            .call(this.client.createCompletion.bind(this.client), request, axiosOptions)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n}\n/**\n * PromptLayer wrapper to OpenAI\n * @augments OpenAI\n */\nexport class PromptLayerOpenAI extends OpenAI {\n    get lc_secrets() {\n        return {\n            promptLayerApiKey: \"PROMPTLAYER_API_KEY\",\n        };\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"promptLayerApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"plTags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnPromptLayerId\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.plTags = fields?.plTags ?? [];\n        this.promptLayerApiKey =\n            fields?.promptLayerApiKey ??\n                getEnvironmentVariable(\"PROMPTLAYER_API_KEY\");\n        this.returnPromptLayerId = fields?.returnPromptLayerId;\n        if (!this.promptLayerApiKey) {\n            throw new Error(\"Missing PromptLayer API key\");\n        }\n    }\n    async completionWithRetry(request, options) {\n        if (request.stream) {\n            return super.completionWithRetry(request, options);\n        }\n        const response = await super.completionWithRetry(request);\n        return response;\n    }\n    async _generate(prompts, options, runManager) {\n        const requestStartTime = Date.now();\n        const generations = await super._generate(prompts, options, runManager);\n        for (let i = 0; i < generations.generations.length; i += 1) {\n            const requestEndTime = Date.now();\n            const parsedResp = {\n                text: generations.generations[i][0].text,\n                llm_output: generations.llmOutput,\n            };\n            const promptLayerRespBody = await promptLayerTrackRequest(this.caller, \"langchain.PromptLayerOpenAI\", [prompts[i]], this._identifyingParams(), this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n            let promptLayerRequestId;\n            if (this.returnPromptLayerId === true) {\n                if (promptLayerRespBody && promptLayerRespBody.success === true) {\n                    promptLayerRequestId = promptLayerRespBody.request_id;\n                }\n                generations.generations[i][0].generationInfo = {\n                    ...generations.generations[i][0].generationInfo,\n                    promptLayerRequestId,\n                };\n            }\n        }\n        return generations;\n    }\n}\nexport { OpenAIChat, PromptLayerOpenAIChat } from \"./openai-chat.js\";\n"],"mappings":"AAAA,SAASA,aAAa,EAAEC,SAAS,QAAS,QAAQ;AAClD,SAASC,kBAAkB,QAAQ,kCAAkC;AACrE,SAASC,eAAe,QAAQ,oBAAoB;AACpD,OAAOC,YAAY,MAAM,gCAAgC;AACzD,SAASC,WAAW,QAAQ,kBAAkB;AAC9C,SAASC,UAAU,QAAQ,kBAAkB;AAC7C,SAASC,sBAAsB,EAAEC,MAAM,QAAQ,gBAAgB;AAC/D,SAASC,uBAAuB,QAAQ,yBAAyB;AACjE,SAASC,6BAA6B,QAAQ,mBAAmB;AACjE,SAASC,OAAO,QAAQ,WAAW;AACnC,SAASC,UAAU,QAAQ,kBAAkB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,MAAM,SAASF,OAAO,CAAC;EAChC,OAAOG,OAAOA,CAAA,EAAG;IACb,OAAO,QAAQ;EACnB;EACA,IAAIC,QAAQA,CAAA,EAAG;IACX,OAAO,CAAC,GAAG,KAAK,CAACA,QAAQ,EAAE,SAAS,CAAC;EACzC;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,YAAY,EAAE,gBAAgB;MAC9BC,iBAAiB,EAAE,sBAAsB;MACzCC,YAAY,EAAE;IAClB,CAAC;EACL;EACA,IAAIC,UAAUA,CAAA,EAAG;IACb,OAAO;MACHC,SAAS,EAAE,OAAO;MAClBJ,YAAY,EAAE,gBAAgB;MAC9BK,qBAAqB,EAAE,0BAA0B;MACjDJ,iBAAiB,EAAE,sBAAsB;MACzCK,0BAA0B,EAAE,gCAAgC;MAC5DC,4BAA4B,EAAE;IAClC,CAAC;EACL;EACAC,WAAWA,CAACC,MAAM,EAClB;EACAC,aAAa,EAAE;IACX,IAAID,MAAM,EAAEL,SAAS,EAAEO,UAAU,CAAC,eAAe,CAAC,IAC9CF,MAAM,EAAEL,SAAS,EAAEO,UAAU,CAAC,OAAO,CAAC,IACtCF,MAAM,EAAEL,SAAS,EAAEO,UAAU,CAAC,WAAW,CAAC,EAAE;MAC5C;MACA,OAAO,IAAIhB,UAAU,CAACc,MAAM,EAAEC,aAAa,CAAC;IAChD;IACA,KAAK,CAACD,MAAM,IAAI,CAAC,CAAC,CAAC;IACnBG,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,kBAAkB,EAAE;MAC5CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,GAAG,EAAE;MAC7BC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,aAAa,EAAE;MACvCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,SAAS,EAAE;MACnCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,MAAM,EAAE;MAChCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,WAAW,EAAE;MACrCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,uBAAuB,EAAE;MACjDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,4BAA4B,EAAE;MACtDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,8BAA8B,EAAE;MACxDC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,qBAAqB,EAAE;MAC/CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,cAAc,EAAE;MACxCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAACjB,YAAY,GACbS,MAAM,EAAET,YAAY,IAAIV,sBAAsB,CAAC,gBAAgB,CAAC;IACpE,IAAI,CAACW,iBAAiB,GAClBQ,MAAM,EAAER,iBAAiB,IACrBX,sBAAsB,CAAC,sBAAsB,CAAC;IACtD,IAAI,CAAC,IAAI,CAACW,iBAAiB,IAAI,CAAC,IAAI,CAACD,YAAY,EAAE;MAC/C,MAAM,IAAIkB,KAAK,CAAC,0CAA0C,CAAC;IAC/D;IACA,IAAI,CAACZ,0BAA0B,GAC3BG,MAAM,EAAEH,0BAA0B,IAC9BhB,sBAAsB,CAAC,gCAAgC,CAAC;IAChE,IAAI,CAACiB,4BAA4B,GAC7B,CAACE,MAAM,EAAEU,uCAAuC,IAC5CV,MAAM,EAAEF,4BAA4B,MACnCjB,sBAAsB,CAAC,8CAA8C,CAAC,IACnEA,sBAAsB,CAAC,kCAAkC,CAAC,CAAC;IACvE,IAAI,CAACe,qBAAqB,GACtBI,MAAM,EAAEJ,qBAAqB,IACzBf,sBAAsB,CAAC,0BAA0B,CAAC;IAC1D,IAAI,CAAC8B,mBAAmB,GACpBX,MAAM,EAAEW,mBAAmB,IACvB9B,sBAAsB,CAAC,wBAAwB,CAAC;IACxD,IAAI,CAACY,YAAY,GACbO,MAAM,EAAEC,aAAa,EAAER,YAAY,IAC/BZ,sBAAsB,CAAC,qBAAqB,CAAC;IACrD,IAAI,CAACc,SAAS,GAAGK,MAAM,EAAEL,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACiB,WAAW,GAAGZ,MAAM,EAAEY,WAAW,IAAI,CAAC,CAAC;IAC5C,IAAI,CAACC,SAAS,GAAGb,MAAM,EAAEa,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,OAAO,GAAGd,MAAM,EAAEc,OAAO;IAC9B,IAAI,CAACC,WAAW,GAAGf,MAAM,EAAEe,WAAW,IAAI,IAAI,CAACA,WAAW;IAC1D,IAAI,CAACC,SAAS,GAAGhB,MAAM,EAAEgB,SAAS,IAAI,IAAI,CAACA,SAAS;IACpD,IAAI,CAACC,IAAI,GAAGjB,MAAM,EAAEiB,IAAI,IAAI,IAAI,CAACA,IAAI;IACrC,IAAI,CAACC,gBAAgB,GAAGlB,MAAM,EAAEkB,gBAAgB,IAAI,IAAI,CAACA,gBAAgB;IACzE,IAAI,CAACC,eAAe,GAAGnB,MAAM,EAAEmB,eAAe,IAAI,IAAI,CAACA,eAAe;IACtE,IAAI,CAACC,CAAC,GAAGpB,MAAM,EAAEoB,CAAC,IAAI,IAAI,CAACA,CAAC;IAC5B,IAAI,CAACC,MAAM,GAAGrB,MAAM,EAAEqB,MAAM,IAAI,IAAI,CAACA,MAAM;IAC3C,IAAI,CAACC,SAAS,GAAGtB,MAAM,EAAEsB,SAAS;IAClC,IAAI,CAACC,IAAI,GAAGvB,MAAM,EAAEuB,IAAI;IACxB,IAAI,CAACC,IAAI,GAAGxB,MAAM,EAAEwB,IAAI;IACxB,IAAI,CAACC,SAAS,GAAGzB,MAAM,EAAEyB,SAAS,IAAI,KAAK;IAC3C,IAAI,IAAI,CAACA,SAAS,IAAI,IAAI,CAACJ,MAAM,IAAI,IAAI,CAACA,MAAM,GAAG,CAAC,EAAE;MAClD,MAAM,IAAIZ,KAAK,CAAC,uCAAuC,CAAC;IAC5D;IACA,IAAI,IAAI,CAACjB,iBAAiB,EAAE;MACxB,IAAI,CAAC,IAAI,CAACK,0BAA0B,IAAI,CAAC,IAAI,CAACc,mBAAmB,EAAE;QAC/D,MAAM,IAAIF,KAAK,CAAC,0CAA0C,CAAC;MAC/D;MACA,IAAI,CAAC,IAAI,CAACX,4BAA4B,EAAE;QACpC,MAAM,IAAIW,KAAK,CAAC,4CAA4C,CAAC;MACjE;MACA,IAAI,CAAC,IAAI,CAACb,qBAAqB,EAAE;QAC7B,MAAM,IAAIa,KAAK,CAAC,oCAAoC,CAAC;MACzD;IACJ;IACA,IAAI,CAACiB,YAAY,GAAG;MAChBC,MAAM,EAAE,IAAI,CAACpC,YAAY;MACzBE,YAAY,EAAE,IAAI,CAACA,YAAY;MAC/B,GAAGQ,aAAa;MAChB,GAAGD,MAAM,EAAEC;IACf,CAAC;EACL;EACA;AACJ;AACA;EACI2B,gBAAgBA,CAACC,OAAO,EAAE;IACtB,OAAO;MACHC,KAAK,EAAE,IAAI,CAACnC,SAAS;MACrBoB,WAAW,EAAE,IAAI,CAACA,WAAW;MAC7BgB,UAAU,EAAE,IAAI,CAACf,SAAS;MAC1BgB,KAAK,EAAE,IAAI,CAACf,IAAI;MAChBgB,iBAAiB,EAAE,IAAI,CAACf,gBAAgB;MACxCgB,gBAAgB,EAAE,IAAI,CAACf,eAAe;MACtCC,CAAC,EAAE,IAAI,CAACA,CAAC;MACTe,OAAO,EAAE,IAAI,CAACd,MAAM;MACpBe,UAAU,EAAE,IAAI,CAACd,SAAS;MAC1BC,IAAI,EAAEM,OAAO,EAAEN,IAAI,IAAI,IAAI,CAACA,IAAI;MAChCC,IAAI,EAAE,IAAI,CAACA,IAAI;MACfa,MAAM,EAAE,IAAI,CAACZ,SAAS;MACtB,GAAG,IAAI,CAACb;IACZ,CAAC;EACL;EACA0B,kBAAkBA,CAAA,EAAG;IACjB,OAAO;MACHC,UAAU,EAAE,IAAI,CAAC5C,SAAS;MAC1B,GAAG,IAAI,CAACiC,gBAAgB,CAAC,CAAC;MAC1B,GAAG,IAAI,CAACF;IACZ,CAAC;EACL;EACA;AACJ;AACA;EACIc,iBAAiBA,CAAA,EAAG;IAChB,OAAO,IAAI,CAACF,kBAAkB,CAAC,CAAC;EACpC;EACA;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;EACI,MAAMG,SAASA,CAACC,OAAO,EAAEb,OAAO,EAAEc,UAAU,EAAE;IAC1C,MAAMC,UAAU,GAAGhE,UAAU,CAAC8D,OAAO,EAAE,IAAI,CAAC7B,SAAS,CAAC;IACtD,MAAMgC,OAAO,GAAG,EAAE;IAClB,MAAMC,UAAU,GAAG,CAAC,CAAC;IACrB,MAAMC,MAAM,GAAG,IAAI,CAACnB,gBAAgB,CAACC,OAAO,CAAC;IAC7C,IAAIkB,MAAM,CAAChB,UAAU,KAAK,CAAC,CAAC,EAAE;MAC1B,IAAIW,OAAO,CAACM,MAAM,KAAK,CAAC,EAAE;QACtB,MAAM,IAAIvC,KAAK,CAAC,wDAAwD,CAAC;MAC7E;MACAsC,MAAM,CAAChB,UAAU,GAAG,MAAMvD,kBAAkB,CAAC;QACzCyE,MAAM,EAAEP,OAAO,CAAC,CAAC,CAAC;QAClB;QACA/C,SAAS,EAAE,IAAI,CAACA;MACpB,CAAC,CAAC;IACN;IACA,KAAK,IAAIuD,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGN,UAAU,CAACI,MAAM,EAAEE,CAAC,IAAI,CAAC,EAAE;MAC3C,MAAMC,IAAI,GAAGJ,MAAM,CAACV,MAAM,GACpB,MAAM,IAAIe,OAAO,CAAC,CAACC,OAAO,EAAEC,MAAM,KAAK;QACrC,MAAMT,OAAO,GAAG,EAAE;QAClB,IAAIU,QAAQ;QACZ,IAAIC,QAAQ,GAAG,KAAK;QACpB,IAAIC,QAAQ,GAAG,KAAK;QACpB,IAAI,CAACC,mBAAmB,CAAC;UACrB,GAAGX,MAAM;UACTE,MAAM,EAAEL,UAAU,CAACM,CAAC;QACxB,CAAC,EAAE;UACCS,MAAM,EAAE9B,OAAO,CAAC8B,MAAM;UACtB,GAAG9B,OAAO,CAACA,OAAO;UAClB+B,OAAO,EAAElF,YAAY;UACrBmF,YAAY,EAAE,QAAQ;UACtBC,SAAS,EAAGC,KAAK,IAAK;YAClB,IAAIA,KAAK,CAACZ,IAAI,EAAEa,IAAI,GAAG,CAAC,KAAK,QAAQ,EAAE;cACnC,IAAIP,QAAQ,IAAID,QAAQ,EAAE;gBACtB;cACJ;cACAC,QAAQ,GAAG,IAAI;cACfJ,OAAO,CAAC;gBACJ,GAAGE,QAAQ;gBACXV;cACJ,CAAC,CAAC;YACN,CAAC,MACI;cACD,MAAMM,IAAI,GAAGc,IAAI,CAACC,KAAK,CAACH,KAAK,CAACZ,IAAI,CAAC;cACnC,IAAIA,IAAI,EAAEgB,KAAK,EAAE;gBACb,IAAIX,QAAQ,EAAE;kBACV;gBACJ;gBACAA,QAAQ,GAAG,IAAI;gBACfF,MAAM,CAACH,IAAI,CAACgB,KAAK,CAAC;gBAClB;cACJ;cACA,MAAMC,OAAO,GAAGjB,IAAI;cACpB;cACA,IAAI,CAACI,QAAQ,EAAE;gBACXA,QAAQ,GAAG;kBACPc,EAAE,EAAED,OAAO,CAACC,EAAE;kBACdC,MAAM,EAAEF,OAAO,CAACE,MAAM;kBACtBC,OAAO,EAAEH,OAAO,CAACG,OAAO;kBACxBzC,KAAK,EAAEsC,OAAO,CAACtC;gBACnB,CAAC;cACL;cACA;cACA,KAAK,MAAM0C,IAAI,IAAIJ,OAAO,CAACvB,OAAO,EAAE;gBAChC,IAAI2B,IAAI,IAAI,IAAI,IAAIA,IAAI,CAACC,KAAK,IAAI,IAAI,EAAE;kBACpC,IAAI,CAAC5B,OAAO,CAAC2B,IAAI,CAACC,KAAK,CAAC,EACpB5B,OAAO,CAAC2B,IAAI,CAACC,KAAK,CAAC,GAAG,CAAC,CAAC;kBAC5B,MAAMC,MAAM,GAAG7B,OAAO,CAAC2B,IAAI,CAACC,KAAK,CAAC;kBAClCC,MAAM,CAACC,IAAI,GAAG,CAACD,MAAM,CAACC,IAAI,IAAI,EAAE,KAAKH,IAAI,CAACG,IAAI,IAAI,EAAE,CAAC;kBACrDD,MAAM,CAACE,aAAa,GAAGJ,IAAI,CAACI,aAAa;kBACzCF,MAAM,CAACG,QAAQ,GAAGL,IAAI,CAACK,QAAQ;kBAC/B;kBACA,KAAKlC,UAAU,EAAEmC,iBAAiB,CAACN,IAAI,CAACG,IAAI,IAAI,EAAE,EAAE;oBAChD1B,MAAM,EAAE8B,IAAI,CAACC,KAAK,CAACR,IAAI,CAACC,KAAK,GAAG,IAAI,CAACrD,CAAC,CAAC;oBACvC6D,UAAU,EAAET,IAAI,CAACC,KAAK,GAAG,IAAI,CAACrD;kBAClC,CAAC,CAAC;gBACN;cACJ;cACA;cACA,IAAI,CAACqC,QAAQ,IACT,CAACD,QAAQ,IACTX,OAAO,CAACqC,KAAK,CAAEC,CAAC,IAAKA,CAAC,CAACP,aAAa,IAAI,IAAI,CAAC,EAAE;gBAC/CnB,QAAQ,GAAG,IAAI;gBACfJ,OAAO,CAAC;kBACJ,GAAGE,QAAQ;kBACXV;gBACJ,CAAC,CAAC;cACN;YACJ;UACJ;QACJ,CAAC,CAAC,CAACuC,KAAK,CAAEjB,KAAK,IAAK;UAChB,IAAI,CAACX,QAAQ,EAAE;YACXA,QAAQ,GAAG,IAAI;YACfF,MAAM,CAACa,KAAK,CAAC;UACjB;QACJ,CAAC,CAAC;MACN,CAAC,CAAC,GACA,MAAM,IAAI,CAACT,mBAAmB,CAAC;QAC7B,GAAGX,MAAM;QACTE,MAAM,EAAEL,UAAU,CAACM,CAAC;MACxB,CAAC,EAAE;QACCS,MAAM,EAAE9B,OAAO,CAAC8B,MAAM;QACtB,GAAG9B,OAAO,CAACA;MACf,CAAC,CAAC;MACNgB,OAAO,CAACwC,IAAI,CAAC,GAAGlC,IAAI,CAACN,OAAO,CAAC;MAC7B,MAAM;QAAEyC,iBAAiB,EAAEC,gBAAgB;QAAEC,aAAa,EAAEC,YAAY;QAAEC,YAAY,EAAEC;MAAa,CAAC,GAAGxC,IAAI,CAACyC,KAAK,IAAI,CAAC,CAAC;MACzH,IAAIL,gBAAgB,EAAE;QAClBzC,UAAU,CAACyC,gBAAgB,GACvB,CAACzC,UAAU,CAACyC,gBAAgB,IAAI,CAAC,IAAIA,gBAAgB;MAC7D;MACA,IAAIE,YAAY,EAAE;QACd3C,UAAU,CAAC2C,YAAY,GAAG,CAAC3C,UAAU,CAAC2C,YAAY,IAAI,CAAC,IAAIA,YAAY;MAC3E;MACA,IAAIE,WAAW,EAAE;QACb7C,UAAU,CAAC6C,WAAW,GAAG,CAAC7C,UAAU,CAAC6C,WAAW,IAAI,CAAC,IAAIA,WAAW;MACxE;IACJ;IACA,MAAME,WAAW,GAAGjH,UAAU,CAACiE,OAAO,EAAE,IAAI,CAACzB,CAAC,CAAC,CAAC0E,GAAG,CAAEC,aAAa,IAAKA,aAAa,CAACD,GAAG,CAAEpB,MAAM,KAAM;MAClGC,IAAI,EAAED,MAAM,CAACC,IAAI,IAAI,EAAE;MACvBqB,cAAc,EAAE;QACZC,YAAY,EAAEvB,MAAM,CAACE,aAAa;QAClCC,QAAQ,EAAEH,MAAM,CAACG;MACrB;IACJ,CAAC,CAAC,CAAC,CAAC;IACJ,OAAO;MACHgB,WAAW;MACXK,SAAS,EAAE;QAAEpD;MAAW;IAC5B,CAAC;EACL;EACA;EACA;EACA,OAAOqD,qBAAqBA,CAACC,KAAK,EAAEvE,OAAO,EAAEc,UAAU,EAAE;IACrD,MAAMI,MAAM,GAAG;MACX,GAAG,IAAI,CAACnB,gBAAgB,CAACC,OAAO,CAAC;MACjCoB,MAAM,EAAEmD,KAAK;MACb/D,MAAM,EAAE;IACZ,CAAC;IACD,MAAMgE,cAAc,GAAG,IAAI,CAACC,WAAW,CAACvD,MAAM,EAAElB,OAAO,CAAC;IACxD,WAAW,MAAM0E,gBAAgB,IAAIF,cAAc,EAAE;MACjD,MAAMlD,IAAI,GAAGc,IAAI,CAACC,KAAK,CAACqC,gBAAgB,CAAC;MACzC,MAAM7B,MAAM,GAAGvB,IAAI,CAACN,OAAO,GAAG,CAAC,CAAC;MAChC,IAAI,CAAC6B,MAAM,EAAE;QACT;MACJ;MACA,MAAM8B,KAAK,GAAG,IAAI/H,eAAe,CAAC;QAC9BkG,IAAI,EAAED,MAAM,CAACC,IAAI;QACjBqB,cAAc,EAAE;UACZC,YAAY,EAAEvB,MAAM,CAACE,aAAa;UAClCC,QAAQ,EAAEH,MAAM,CAACG;QACrB;MACJ,CAAC,CAAC;MACF,MAAM2B,KAAK;MACX;MACA,KAAK7D,UAAU,EAAEmC,iBAAiB,CAAC0B,KAAK,CAAC7B,IAAI,IAAI,EAAE,CAAC;IACxD;EACJ;EACA2B,WAAWA,CAACG,OAAO,EAAE5E,OAAO,EAAE;IAC1B,IAAI6E,IAAI,GAAG,KAAK;IAChB,MAAMrE,MAAM,GAAG,IAAIsE,eAAe,CAAC,CAAC;IACpC,MAAMC,MAAM,GAAGvE,MAAM,CAAC9B,QAAQ,CAACsG,SAAS,CAAC,CAAC;IAC1C,MAAMC,QAAQ,GAAG9H,6BAA6B,CAACqD,MAAM,CAAC0E,QAAQ,CAAC;IAC/D;IACA,IAAIC,GAAG;IACP,IAAI,CAACtD,mBAAmB,CAAC+C,OAAO,EAAE;MAC9B,GAAG5E,OAAO;MACV+B,OAAO,EAAElF,YAAY;MACrBmF,YAAY,EAAE,QAAQ;MACtBC,SAAS,EAAGC,KAAK,IAAK;QAClB,IAAI2C,IAAI,EACJ;QACJ,IAAI3C,KAAK,CAACZ,IAAI,EAAEa,IAAI,GAAG,CAAC,KAAK,QAAQ,EAAE;UACnC0C,IAAI,GAAG,IAAI;UACX;UACA,KAAKE,MAAM,CAACK,KAAK,CAAC,CAAC;QACvB,CAAC,MACI;UACD,MAAM9D,IAAI,GAAGc,IAAI,CAACC,KAAK,CAACH,KAAK,CAACZ,IAAI,CAAC;UACnC,IAAIA,IAAI,CAACgB,KAAK,EAAE;YACZuC,IAAI,GAAG,IAAI;YACX,MAAMvD,IAAI,CAACgB,KAAK;UACpB;UACA;UACA,KAAKyC,MAAM,CAACM,KAAK,CAACnD,KAAK,CAACZ,IAAI,CAAC;QACjC;MACJ;IACJ,CAAC,CAAC,CAACiC,KAAK,CAAEjB,KAAK,IAAK;MAChB,IAAI,CAACuC,IAAI,EAAE;QACPM,GAAG,GAAG7C,KAAK;QACXuC,IAAI,GAAG,IAAI;QACX;QACA,KAAKE,MAAM,CAACK,KAAK,CAAC,CAAC;MACvB;IACJ,CAAC,CAAC;IACF,OAAO;MACH,MAAME,IAAIA,CAAA,EAAG;QACT,MAAMX,KAAK,GAAG,MAAMM,QAAQ,CAACK,IAAI,CAAC,CAAC;QACnC,IAAIH,GAAG,EAAE;UACL,MAAMA,GAAG;QACb;QACA,OAAOR,KAAK;MAChB,CAAC;MACD,CAACY,MAAM,CAACC,aAAa,IAAI;QACrB,OAAO,IAAI;MACf;IACJ,CAAC;EACL;EACA;EACA,MAAM3D,mBAAmBA,CAAC+C,OAAO,EAAE5E,OAAO,EAAE;IACxC,IAAI,CAAC,IAAI,CAACyF,MAAM,EAAE;MACd,MAAMC,oBAAoB,GAAG;QACzBzH,4BAA4B,EAAE,IAAI,CAACA,4BAA4B;QAC/DD,0BAA0B,EAAE,IAAI,CAACA,0BAA0B;QAC3DL,iBAAiB,EAAE,IAAI,CAACA,iBAAiB;QACzCmB,mBAAmB,EAAE,IAAI,CAACA,mBAAmB;QAC7C6G,QAAQ,EAAE,IAAI,CAAC9F,YAAY,CAAC8F;MAChC,CAAC;MACD,MAAMC,QAAQ,GAAG9I,WAAW,CAAC4I,oBAAoB,CAAC;MAClD,MAAM7F,YAAY,GAAG,IAAIpD,aAAa,CAAC;QACnC,GAAG,IAAI,CAACoD,YAAY;QACpB8F,QAAQ,EAAEC,QAAQ;QAClBC,WAAW,EAAE;UACT5G,OAAO,EAAE,IAAI,CAACA,OAAO;UACrB,GAAG,IAAI,CAACY,YAAY,CAACgG;QACzB;MACJ,CAAC,CAAC;MACF,IAAI,CAACJ,MAAM,GAAG,IAAI/I,SAAS,CAACmD,YAAY,CAAC;IAC7C;IACA,MAAMiG,YAAY,GAAG;MACjB/D,OAAO,EAAE9E,MAAM,CAAC,CAAC,GAAG8I,SAAS,GAAGlJ,YAAY;MAC5C,GAAG,IAAI,CAACgD,YAAY,CAACgG,WAAW;MAChC,GAAG7F;IACP,CAAC;IACD,IAAI,IAAI,CAACrC,iBAAiB,EAAE;MACxBmI,YAAY,CAACE,OAAO,GAAG;QACnB,SAAS,EAAE,IAAI,CAACrI,iBAAiB;QACjC,GAAGmI,YAAY,CAACE;MACpB,CAAC;MACDF,YAAY,CAAC5E,MAAM,GAAG;QAClB,aAAa,EAAE,IAAI,CAACnD,qBAAqB;QACzC,GAAG+H,YAAY,CAAC5E;MACpB,CAAC;IACL;IACA,OAAO,IAAI,CAAC+E,MAAM,CACbC,IAAI,CAAC,IAAI,CAACT,MAAM,CAACU,gBAAgB,CAACC,IAAI,CAAC,IAAI,CAACX,MAAM,CAAC,EAAEb,OAAO,EAAEkB,YAAY,CAAC,CAC3EO,IAAI,CAAEC,GAAG,IAAKA,GAAG,CAAChF,IAAI,CAAC;EAChC;EACAiF,QAAQA,CAAA,EAAG;IACP,OAAO,QAAQ;EACnB;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMC,iBAAiB,SAASlJ,MAAM,CAAC;EAC1C,IAAIG,UAAUA,CAAA,EAAG;IACb,OAAO;MACHgJ,iBAAiB,EAAE;IACvB,CAAC;EACL;EACAvI,WAAWA,CAACC,MAAM,EAAE;IAChB,KAAK,CAACA,MAAM,CAAC;IACbG,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,iBAAiB,EAAE;MAC3CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE;IACX,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,mBAAmB,EAAE;MAC7CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,QAAQ,EAAE;MAClCC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACFL,MAAM,CAACC,cAAc,CAAC,IAAI,EAAE,qBAAqB,EAAE;MAC/CC,UAAU,EAAE,IAAI;MAChBC,YAAY,EAAE,IAAI;MAClBC,QAAQ,EAAE,IAAI;MACdC,KAAK,EAAE,KAAK;IAChB,CAAC,CAAC;IACF,IAAI,CAAC+H,MAAM,GAAGvI,MAAM,EAAEuI,MAAM,IAAI,EAAE;IAClC,IAAI,CAACD,iBAAiB,GAClBtI,MAAM,EAAEsI,iBAAiB,IACrBzJ,sBAAsB,CAAC,qBAAqB,CAAC;IACrD,IAAI,CAAC2J,mBAAmB,GAAGxI,MAAM,EAAEwI,mBAAmB;IACtD,IAAI,CAAC,IAAI,CAACF,iBAAiB,EAAE;MACzB,MAAM,IAAI7H,KAAK,CAAC,6BAA6B,CAAC;IAClD;EACJ;EACA,MAAMiD,mBAAmBA,CAAC+C,OAAO,EAAE5E,OAAO,EAAE;IACxC,IAAI4E,OAAO,CAACpE,MAAM,EAAE;MAChB,OAAO,KAAK,CAACqB,mBAAmB,CAAC+C,OAAO,EAAE5E,OAAO,CAAC;IACtD;IACA,MAAM0B,QAAQ,GAAG,MAAM,KAAK,CAACG,mBAAmB,CAAC+C,OAAO,CAAC;IACzD,OAAOlD,QAAQ;EACnB;EACA,MAAMd,SAASA,CAACC,OAAO,EAAEb,OAAO,EAAEc,UAAU,EAAE;IAC1C,MAAM8F,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAAC,CAAC;IACnC,MAAM9C,WAAW,GAAG,MAAM,KAAK,CAACpD,SAAS,CAACC,OAAO,EAAEb,OAAO,EAAEc,UAAU,CAAC;IACvE,KAAK,IAAIO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2C,WAAW,CAACA,WAAW,CAAC7C,MAAM,EAAEE,CAAC,IAAI,CAAC,EAAE;MACxD,MAAM0F,cAAc,GAAGF,IAAI,CAACC,GAAG,CAAC,CAAC;MACjC,MAAME,UAAU,GAAG;QACflE,IAAI,EAAEkB,WAAW,CAACA,WAAW,CAAC3C,CAAC,CAAC,CAAC,CAAC,CAAC,CAACyB,IAAI;QACxCmE,UAAU,EAAEjD,WAAW,CAACK;MAC5B,CAAC;MACD,MAAM6C,mBAAmB,GAAG,MAAMhK,uBAAuB,CAAC,IAAI,CAAC+I,MAAM,EAAE,6BAA6B,EAAE,CAACpF,OAAO,CAACQ,CAAC,CAAC,CAAC,EAAE,IAAI,CAACZ,kBAAkB,CAAC,CAAC,EAAE,IAAI,CAACiG,MAAM,EAAEM,UAAU,EAAEJ,gBAAgB,EAAEG,cAAc,EAAE,IAAI,CAACN,iBAAiB,CAAC;MACjO,IAAIU,oBAAoB;MACxB,IAAI,IAAI,CAACR,mBAAmB,KAAK,IAAI,EAAE;QACnC,IAAIO,mBAAmB,IAAIA,mBAAmB,CAACE,OAAO,KAAK,IAAI,EAAE;UAC7DD,oBAAoB,GAAGD,mBAAmB,CAACG,UAAU;QACzD;QACArD,WAAW,CAACA,WAAW,CAAC3C,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC8C,cAAc,GAAG;UAC3C,GAAGH,WAAW,CAACA,WAAW,CAAC3C,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC8C,cAAc;UAC/CgD;QACJ,CAAC;MACL;IACJ;IACA,OAAOnD,WAAW;EACtB;AACJ;AACA,SAAS3G,UAAU,EAAEiK,qBAAqB,QAAQ,kBAAkB"},"metadata":{},"sourceType":"module","externalDependencies":[]}